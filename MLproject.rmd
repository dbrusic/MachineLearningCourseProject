---
title: "Machine Learning Course Project"
author: "Dan Brusic"
date: "January 2018"
output: html_document
---

## Overview
The data set for this project comes from this [website](http://groupware.les.inf.puc-rio.br/har). It includes information obtained from on-body sensors on six participants who each performed a weight lifting exercise. The exercise was performed correctly (class A in the classe column) and incorrectly in 4 different ways (classes B, C, D, and E). The goal is to fit a model to the dataset that will accurately predict which version of the exercise was performed (or in other words, if the exercise was performed correctly or not). A random forest model and gradient boosting model were compared using the `caret` package and 5-fold cross validation. Accuracy was used to pick the best model, which in the end was the random forest. Further information can be found in this [paper](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201).
  
  
## Downloading the Data
For this project there are two data sets. The training data set contains the data that we will build and test the models on. The testing data set only contains 20 rows and excludes the response variable (classe). The testing data set will only be used for a quiz at the end of the project.
```{r cache=TRUE}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                     stringsAsFactors = FALSE,
                     na.strings = c("NA", "", "#DIV/0!"))
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                    stringsAsFactors = FALSE,
                    na.strings = c("NA", "", "#DIV/0!"))
```

## Exploring the Data
The response variable (classe) and a predictor variable (user_name) are changed to factor variables. For the purposes of this project we will use the `user_name` column because it most likely will improve the accuracy of our models (given that it represents different performers of the exercise). A more practical model might exclude this column to see how well the model can predict exercise performance regardless of the performer. 
```{r}
training$classe <- factor(training$classe)
training$user_name <- factor(training$user_name)
testing$user_name <- factor(testing$user_name)
```

Below we create a table of the response variable. The five classes seem to be represented fairly well in the data, so our models will most likely not be affected by overly expressed classes.

```{r}
table(training$classe)
```

Some of the columns have almost all missing values. We will simply exclude these columns rather than impute. The unique amounts of missing values tells us that all the columns that have missing values are almost completely missing.

```{r}
sum_missing <- colSums(is.na(training))
sum_missing <- sum_missing[sum_missing != 0]
unique(sum_missing)
```

## Prepairing Data for Model Fitting
Below we remove the NA columns and the timestamp columns. They will not be needed for training our models as we will only be using the data obtained from the body sensors.

```{r}
NA_cols <- which(apply(is.na(training), 2, any))
training.reduced <- training[, -c(1,3,4,5,6,7, NA_cols)]
testing.reduced <- testing[, -c(1,3,4,5,6,7, NA_cols)]
```

Below we check for variables that have near zero and zero variance. However, when using the defaults of the `nearZeroVar` caret function (freqCut = 19 and uniqueCut = 10) none of the variables have near zero variance, so all variables will be used when training the models. 

```{r}
library(caret)
zerVar <- nearZeroVar(training.reduced)
length(zerVar)
```

Below we split `training.reduced` into train and test sets. The train set is 80% of the full training set and the test set is 20%.

```{r}
set.seed(4444)
rows <- sample(nrow(training.reduced)) # shuffle rows
training.reduced <- training.reduced[rows, ]
split <- round(nrow(training.reduced)*0.80) # 80/20 split
train <- training.reduced[1:split, ]
test <- training.reduced[(split+1):nrow(training.reduced), ]
```

Below we separate the response column from the predictor columns (to be used when building our models). We also create the five folds of the data that will be used for cross validation for both of our models. This is done here so that the folds are the same for both the random forest model and the gradient boosting model. This allows for a fair comparison later on.

```{r}
library(ranger)
library(gbm)
train_x <- train[,-54]
train_y <- train[, 54] # classe
set.seed(4444)
myFolds <- createFolds(train_y, k = 5)
myControl <- trainControl(method = "cv",
                          number = 5,
                          savePredictions = TRUE,
                          index = myFolds,
                          summaryFunction = defaultSummary) # just use accuracy to determine best model
```

## Model Fitting and Selection
Now we can fit a random forest model with the `caret` package using the `ranger` package. The ranger package was used instead of the `randomForest` package because it tends to run faster. We use the control we created above that specifies to use 5-fold cross validation and accuracy to select the best model (based on differing default tuning parameters).

```{r cache=TRUE}
set.seed(4444)
model_rf <- train(x = train_x, y = train_y, method = "ranger", trControl = myControl)
```

Below we print and plot the model and can see that the selected one use mtry set to 53. So 53 variables are randomly selected at each node of each tree (as opposed to using 2 or 27).

```{r}
model_rf
plot(model_rf)
```

Now we fit the gradient boosting model with the same control that we used for the random forest (including the cross validation data). 

```{r cache=TRUE}
set.seed(4444)
model_gbm <- train(x = train_x, y = train_y, method = "gbm", trControl = myControl, verbose = FALSE)
```

Below we print and plot the model and can see that the final model has 150 trees and an interaction depth of 3. 

```{r}
model_gbm
plot(model_gbm)
```


## Model Selection
Using 5-fold cross validation we can directly compare how each model performs using the `resamples` function from `caret`. As can be seen in the summary of the resamples and in the box and whiskers plot, the random forest has a higher median accuracy at 96.97% (vs 94.34% for gradient boosting). In this case we select the random forest model to make predictions on the test set created earlier.

```{r}
model_list <- list(rf = model_rf, gbm = model_gbm)
resamples <- resamples(model_list)
summary(resamples)
bwplot(resamples, metric = "Accuracy", main = "Comparison of Model Accuracy\n (5-Fold Cross Validation)")
```

Below we further test the random forest model's out of sample accuracy and make sure it is not overfitting the data. As can be seen in the confusion matrix output, the model performs even better than the cross validation, with a prediction accuracy on the test set of 99.62%.  

```{r}
p <- predict(model_rf, test, type = "raw")
confusionMatrix(p, test$classe)
```

## Quiz Predictions

```{r}
quiz.p <- predict(model_rf, testing, type = "raw")
quiz.p
```

## Conclusion
The random forest model that uses 53 random variables at each node outperformed the gradient boosting model. It's estimated out of sample error (based on predicting the test set) was 99.62%. The 5-fold cross validation was used to test the models to reduce model fitting time (instead of bootstrapping). In the end, very accurate models were produced.